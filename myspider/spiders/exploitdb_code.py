# -*- coding: utf-8 -*-
import scrapy
from myspider.items import ExploitDBItem,ExploitFileItem
import os,json

class ExploitdbSpider(scrapy.Spider):
    name = 'exploitdb'
    allowed_domains = ['www.exploit-db.com']

    DEFAULT_REQUEST_HEADERS = {
        'Accept': 'application/json, text/javascript, */*; q=0.01',
        'Accept-Language': 'en',
        'x-requested-with': 'XMLHttpRequest'
    }

    DEFAULT_LENGTH = 1200

    def start_requests(self):
        cls = self.__class__
        start_urls = ['https://www.exploit-db.com/?draw=1&start={}&length={}'.format(0,cls.DEFAULT_LENGTH)
                      ]
        # return [scrapy.FormRequest('https://www.exploit-db.com/exploits/44218/')]

        return [scrapy.FormRequest("{}".format(url),headers=cls.DEFAULT_REQUEST_HEADERS,
                                   callback=self.parse_list) for url in start_urls]

    def parse(self, response):

        item = ExploitDBItem()
        str_value = lambda x :x.replace('\n','').replace(':','',1).strip() if x is not None else None

        exploit_table = response.xpath('//div[contains(@class,"exploit-meta")]/div[1]/table')
        edb_id = exploit_table.xpath('tr[1]/td[1]/text()').extract_first()
        author = exploit_table.xpath('tr[1]/td[2]/a/text()').extract_first()
        published = exploit_table.xpath('tr[1]/td[3]/text()').extract_first()
        cve_id = exploit_table.xpath('tr[2]/td[1]/a/text()').extract_first()
        vuln_type = exploit_table.xpath('tr[2]/td[2]/a/text()').extract_first()
        platform = exploit_table.xpath('tr[2]/td[3]/a/text()').extract_first()
        if len(exploit_table.xpath('tr').extract())==3:
            vuln_app = ''.join(exploit_table.xpath('tr[3]/td[3]/text()').extract())
            verified = exploit_table.xpath('tr[3]/td[1]/a/img/@title').extract_first()
            aliases=''
            advisory=''
            tags=''
        else:
            aliases = exploit_table.xpath('tr[3]/td[1]/text()').extract_first()
            advisory = exploit_table.xpath('tr[3]/td[2]/a/@href').extract_first()
            tags = ''.join(exploit_table.xpath('tr[3]/td[3]/text()').extract())

            vuln_app = ''.join(exploit_table.xpath('tr[4]/td[3]/text()').extract())
            verified = exploit_table.xpath('tr[4]/td[1]/a/img/@title').extract_first()

        item['edb_id'] = str_value(edb_id)
        item['author'] = str_value(author)
        item['published'] = str_value(published)
        item['cve_id'] = str_value(cve_id)
        item['vuln_type'] = str_value(vuln_type)
        item['platform'] = str_value(platform)
        item['aliases'] = str_value(aliases)
        item['advisory'] = str_value(advisory)
        item['tags'] = str_value(tags)
        item['verified'] = str_value(verified)
        item['vuln_app'] = str_value(vuln_app)

        exploit_code = response.xpath('//div[@id="container"]/pre/text()').extract_first()
        if not os.path.exists('exploit-code'):
            os.mkdir('exploit-code')
        try:
            filename = os.path.join('./exploit-code',item['edb_id'])
            with open(filename, 'wb') as f:
                f.write(exploit_code)
        except Exception as ex:
            with open('error.log','a+') as f:
                f.write(response.url)
                f.write('\n')

        related_table = response.xpath('//div[contains(@class,"l-section-h")]/table[contains(@class,"exploit_list")]/tbody/tr')
        related_exploits = []
        for tr in related_table:
            exp = {
                'date' : str_value(tr.xpath('td[1]/text()').extract_first()),
                'verification':tr.xpath('td[3]/a/img/@title').extract_first(),
                'title':tr.xpath('td[4]/a/text()').extract_first(),
                'link':tr.xpath('td[4]/a/@href').extract_first(),
                'author':tr.xpath('td[5]/a/text()').extract_first(),
                'author_link':tr.xpath('td[5]/a/@href').extract_first()
            }
            related_exploits.append(exp)

        item['related_exploits'] = related_exploits
        print item
        yield item

    def parse_list(self, response):
        print response.text
        cls = self.__class__

        try:
            resp_json = json.loads(response.text)
            draw = resp_json["draw"]
            recordsTotal = resp_json['recordsTotal']
            data = resp_json['data']
            if len(data) > 0:
                for d in data:
                    item = ExploitDBItem()
                    for k in d.keys():
                        print k,item.keys()
                        # if k in item.keys():
                        item[k] = d[k]
                        # else:
                        #     print "{} is not in exploit item".format(k)
                    yield item
                    if 'download' in d:
                        download = d['download']
                        fileItem = ExploitFileItem()
                        fileItem['file_urls'] = ['https://www.exploit-db.com/download/{}'.format(d['id'])]
                        # yield fileItem

            if draw * cls.DEFAULT_LENGTH < recordsTotal:
                next_url = 'https://www.exploit-db.com/?draw={draw}&start={start}&length={length}'.format(draw = draw+1,start = draw * cls.DEFAULT_LENGTH,length = cls.DEFAULT_LENGTH)
                yield scrapy.FormRequest("{}".format(next_url), headers=cls.DEFAULT_REQUEST_HEADERS,
                                   callback=self.parse_list)
        except Exception as ex:
            print ex
        # exploit_links = response.xpath('//td[@class="description"]/a/@href').extract()
        # for elink in exploit_links:
        #     yield scrapy.Request(url=elink)
        #
        # link_page = response.xpath('//div[@class="pagination"]/a[contains(.//text(), "next")]/@href').extract_first()
        # if link_page is not None:
        #     print "next link",link_page
        #     yield scrapy.FormRequest("{}".format(link_page), callback=self.parse_list)