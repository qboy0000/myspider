# -*- coding: utf-8 -*-
import scrapy
from myspider.items import ExploitDBItem
import os

class ExploitdbSpider(scrapy.Spider):
    name = 'exploitdb'
    allowed_domains = ['www.exploit-db.com']


    def start_requests(self):
        start_urls = ['https://www.exploit-db.com/local/',
                      'https://www.exploit-db.com/remote/',
                      'https://www.exploit-db.com/webapps/',
                      'https://www.exploit-db.com/dos/'
                      ]
        # return [scrapy.FormRequest('https://www.exploit-db.com/exploits/44218/')]
        return [scrapy.FormRequest("{}".format(url),
                                   callback=self.parse_list) for url in start_urls]

    def parse(self, response):

        item = ExploitDBItem()
        str_value = lambda x :x.replace('\n','').replace(':','',1).strip() if x is not None else None

        exploit_table = response.xpath('//div[contains(@class,"exploit-meta")]/div[1]/table')
        edb_id = exploit_table.xpath('tr[1]/td[1]/text()').extract_first()
        author = exploit_table.xpath('tr[1]/td[2]/a/text()').extract_first()
        published = exploit_table.xpath('tr[1]/td[3]/text()').extract_first()
        cve_id = exploit_table.xpath('tr[2]/td[1]/a/text()').extract_first()
        vuln_type = exploit_table.xpath('tr[2]/td[2]/a/text()').extract_first()
        platform = exploit_table.xpath('tr[2]/td[3]/a/text()').extract_first()
        if len(exploit_table.xpath('tr').extract())==3:
            vuln_app = ''.join(exploit_table.xpath('tr[3]/td[3]/text()').extract())
            verified = exploit_table.xpath('tr[3]/td[1]/a/img/@title').extract_first()
            aliases=''
            advisory=''
            tags=''
        else:
            aliases = exploit_table.xpath('tr[3]/td[1]/text()').extract_first()
            advisory = exploit_table.xpath('tr[3]/td[2]/a/@href').extract_first()
            tags = ''.join(exploit_table.xpath('tr[3]/td[3]/text()').extract())

            vuln_app = ''.join(exploit_table.xpath('tr[4]/td[3]/text()').extract())
            verified = exploit_table.xpath('tr[4]/td[1]/a/img/@title').extract_first()

        item['edb_id'] = str_value(edb_id)
        item['author'] = str_value(author)
        item['published'] = str_value(published)
        item['cve_id'] = str_value(cve_id)
        item['vuln_type'] = str_value(vuln_type)
        item['platform'] = str_value(platform)
        item['aliases'] = str_value(aliases)
        item['advisory'] = str_value(advisory)
        item['tags'] = str_value(tags)
        item['verified'] = str_value(verified)
        item['vuln_app'] = str_value(vuln_app)

        exploit_code = response.xpath('//div[@id="container"]/pre/text()').extract_first()
        if not os.path.exists('exploit-code'):
            os.mkdir('exploit-code')
        try:
            filename = os.path.join('./exploit-code',item['edb_id'])
            with open(filename, 'wb') as f:
                f.write(exploit_code)
        except Exception as ex:
            with open('error.log','a+') as f:
                f.write(response.url)
                f.write('\n')

        related_table = response.xpath('//div[contains(@class,"l-section-h")]/table[contains(@class,"exploit_list")]/tbody/tr')
        related_exploits = []
        for tr in related_table:
            exp = {
                'date' : str_value(tr.xpath('td[1]/text()').extract_first()),
                'verification':tr.xpath('td[3]/a/img/@title').extract_first(),
                'title':tr.xpath('td[4]/a/text()').extract_first(),
                'link':tr.xpath('td[4]/a/@href').extract_first(),
                'author':tr.xpath('td[5]/a/text()').extract_first(),
                'author_link':tr.xpath('td[5]/a/@href').extract_first()
            }
            related_exploits.append(exp)

        item['related_exploits'] = related_exploits
        print item
        yield item

    def parse_list(self, response):
        exploit_links = response.xpath('//td[@class="description"]/a/@href').extract()
        for elink in exploit_links:
            yield scrapy.Request(url=elink)

        link_page = response.xpath('//div[@class="pagination"]/a[contains(.//text(), "next")]/@href').extract_first()
        if link_page is not None:
            print "next link",link_page
            yield scrapy.FormRequest("{}".format(link_page), callback=self.parse_list)